{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to execute a distributed training workload, connecting the following heterogeneous components:\n",
    "\n",
    "* Preprocessing the dataset with Ray Data\n",
    "* Distributed training of an XGBoost model with Ray Train\n",
    "\n",
    "Note: This tutorial does not cover model tuning. Refer to Ray Tune for experiment execution and hyperparameter tuning at any scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Enable Ray Train V2\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import ray.train.torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, CheckpointConfig, RunConfig, ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "from torch import optim\n",
    "\n",
    "from e2e_timeseries.data_provider.data_factory import data_provider\n",
    "from e2e_timeseries.models import DLinear\n",
    "from e2e_timeseries.utils.metrics import metric\n",
    "from e2e_timeseries.utils.tools import adjust_learning_rate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # basic config\n",
    "    \"train_only\": False,\n",
    "    \"smoke_test\": False,  # Set to True to run a smoke test\n",
    "    # data loader args\n",
    "    \"root_path\": \"./e2e_timeseries/dataset/\",\n",
    "    \"num_data_workers\": 10,\n",
    "    # forecasting task type\n",
    "    # S: univariate predict univariate\n",
    "    # M: multivariate predict univariate\n",
    "    # MS: multivariate predict multivariate\n",
    "    \"features\": \"S\",\n",
    "    \"target\": \"OT\",  # target variable name for prediction\n",
    "    \"checkpoints\": \"./checkpoints/\",\n",
    "    # forecasting task args\n",
    "    \"seq_len\": 96,\n",
    "    \"label_len\": 48,\n",
    "    \"pred_len\": 96,\n",
    "    # DLinear specific args\n",
    "    \"individual\": False,\n",
    "    # optimization args\n",
    "    \"num_replicas\": 1,\n",
    "    \"train_epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"patience\": 3,  # FIXME: early stopping not implemented in this script\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"loss\": \"mse\",\n",
    "    \"lradj\": \"type1\",\n",
    "    \"use_amp\": False,\n",
    "    # Other args\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Set dataset specific args\n",
    "config[\"data\"] = \"ETTh1\"\n",
    "config[\"data_path\"] = \"ETTh1.csv\"\n",
    "if config[\"features\"] == \"S\":  # S: univariate predict univariate\n",
    "    config[\"enc_in\"] = 1\n",
    "else:  # M or MS\n",
    "    config[\"enc_in\"] = 7  # ETTh1 has 7 features\n",
    "\n",
    "# Ensure paths are absolute\n",
    "config[\"root_path\"] = os.path.abspath(config[\"root_path\"])\n",
    "config[\"data_path\"] = os.path.abspath(os.path.join(config[\"root_path\"], config[\"data_path\"]))\n",
    "config[\"checkpoints\"] = os.path.abspath(config[\"checkpoints\"])\n",
    "\n",
    "# --- Smoke Test Modifications ---\n",
    "if config[\"smoke_test\"]:\n",
    "    print(\"--- RUNNING SMOKE TEST ---\")\n",
    "    config[\"train_epochs\"] = 2\n",
    "    config[\"batch_size\"] = 2\n",
    "    config[\"num_data_workers\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ray Train Setup ===\n",
    "ray.init()\n",
    "\n",
    "use_gpu = \"GPU\" in ray.cluster_resources() and ray.cluster_resources()[\"GPU\"] >= 1\n",
    "print(f\"Using GPU: {use_gpu}\")\n",
    "scaling_config = ScalingConfig(num_workers=config[\"num_replicas\"], use_gpu=use_gpu, resources_per_worker={\"GPU\": 1} if use_gpu else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust run name for smoke test\n",
    "run_name_prefix = \"SmokeTest_\" if config[\"smoke_test\"] else \"\"\n",
    "run_name = f\"{run_name_prefix}DLinear_{config['data']}_{config['features']}_{config['target']}_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "run_config = RunConfig(\n",
    "    storage_path=config[\"checkpoints\"],\n",
    "    name=run_name,\n",
    "    checkpoint_config=CheckpointConfig(num_to_keep=2, checkpoint_score_attribute=\"val/loss\", checkpoint_score_order=\"min\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_per_worker(config: dict):\n",
    "    \"\"\"Main training loop adapted for Ray Train workers.\"\"\"\n",
    "\n",
    "    random.seed(config[\"seed\"])\n",
    "    torch.manual_seed(config[\"seed\"])\n",
    "    np.random.seed(config[\"seed\"])\n",
    "\n",
    "    # Automatically determine device based on availability\n",
    "    device = train.torch.get_device()\n",
    "\n",
    "    def _postprocess_preds_and_targets(raw_pred, batch_y, config):\n",
    "        pred_len = config[\"pred_len\"]\n",
    "        f_dim_start_index = -1 if config[\"features\"] == \"MS\" else 0\n",
    "\n",
    "        # Slice for prediction length first\n",
    "        outputs_pred_len = raw_pred[:, -pred_len:, :]\n",
    "        batch_y_pred_len = batch_y[:, -pred_len:, :]\n",
    "\n",
    "        # Then slice for features\n",
    "        final_pred = outputs_pred_len[:, :, f_dim_start_index:]\n",
    "        final_target = batch_y_pred_len[:, :, f_dim_start_index:]\n",
    "\n",
    "        return final_pred, final_target\n",
    "\n",
    "    # === Build Model ===\n",
    "    model = DLinear.Model(config).float()\n",
    "    model = train.torch.prepare_model(model)\n",
    "    model.to(device)\n",
    "\n",
    "    # === Get Data ===\n",
    "    train_ds = data_provider(config, flag=\"train\")\n",
    "    if not config[\"train_only\"]:\n",
    "        val_ds = data_provider(config, flag=\"val\")\n",
    "\n",
    "    # === Optimizer and Criterion ===\n",
    "    model_optim = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # === AMP Scaler ===\n",
    "    scaler = None\n",
    "    if config[\"use_amp\"]:\n",
    "        scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    # === Training Loop ===\n",
    "    for epoch in range(config[\"train_epochs\"]):\n",
    "        model.train()\n",
    "        train_loss_epoch = []\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Iterate over Ray Dataset batches. The dataset now yields dicts {'x': numpy_array, 'y': numpy_array}\n",
    "        # iter_torch_batches will convert these to Torch tensors and move to device.\n",
    "        for batch in train_ds.iter_torch_batches(batch_size=config[\"batch_size\"], device=device, dtypes=torch.float32):\n",
    "            model_optim.zero_grad()\n",
    "            x = batch[\"x\"]\n",
    "            y = batch[\"y\"]\n",
    "\n",
    "            # Forward pass\n",
    "            if config[\"use_amp\"]:\n",
    "                with torch.amp.autocast(\"cuda\"):\n",
    "                    raw_preds = model(x)\n",
    "                    predictions, targets = _postprocess_preds_and_targets(raw_preds, y, config)\n",
    "                    loss = criterion(predictions, targets)\n",
    "            else:\n",
    "                raw_preds = model(x)\n",
    "                predictions, targets = _postprocess_preds_and_targets(raw_preds, y, config)\n",
    "                loss = criterion(predictions, targets)\n",
    "\n",
    "            train_loss_epoch.append(loss.item())\n",
    "\n",
    "            # Backward pass\n",
    "            if config[\"use_amp\"]:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(model_optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                model_optim.step()\n",
    "\n",
    "        # === End of Epoch ===\n",
    "        epoch_train_loss = np.average(train_loss_epoch)\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "        results_dict = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train/loss\": epoch_train_loss,\n",
    "            \"epoch_duration_s\": epoch_duration,\n",
    "        }\n",
    "\n",
    "        # === Validation ===\n",
    "        if not config[\"train_only\"]:\n",
    "            model.eval()\n",
    "            all_preds = []\n",
    "            all_trues = []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_ds.iter_torch_batches(batch_size=config[\"batch_size\"], device=device, dtypes=torch.float32):\n",
    "                    x, y = batch[\"x\"], batch[\"y\"]\n",
    "\n",
    "                    if config[\"use_amp\"] and torch.cuda.is_available():\n",
    "                        with torch.amp.autocast(\"cuda\"):\n",
    "                            raw_preds = model(x)\n",
    "                    else:\n",
    "                        raw_preds = model(x)\n",
    "\n",
    "                    predictions, targets = _postprocess_preds_and_targets(raw_preds, y, config)\n",
    "\n",
    "                    all_preds.append(predictions.detach().cpu().numpy())\n",
    "                    all_trues.append(targets.detach().cpu().numpy())\n",
    "\n",
    "            all_preds = np.concatenate(all_preds, axis=0)\n",
    "            all_trues = np.concatenate(all_trues, axis=0)\n",
    "\n",
    "            mae, mse, rmse, mape, mspe, rse = metric(all_preds, all_trues)\n",
    "\n",
    "            results_dict[\"val/loss\"] = mse\n",
    "            results_dict[\"val/mae\"] = mae\n",
    "            results_dict[\"val/rmse\"] = rmse\n",
    "            results_dict[\"val/mape\"] = mape\n",
    "            results_dict[\"val/mspe\"] = mspe\n",
    "            results_dict[\"val/rse\"] = rse\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}: Train Loss: {epoch_train_loss:.7f}, Val Loss: {mse:.7f}, Val MSE: {mse:.7f} (Duration: {epoch_duration:.2f}s)\")\n",
    "\n",
    "        # === Reporting and Checkpointing ===\n",
    "        if train.get_context().get_world_rank() == 0:\n",
    "            with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": model.module.state_dict() if hasattr(model, \"module\") else model.state_dict(),\n",
    "                        \"optimizer_state_dict\": model_optim.state_dict(),\n",
    "                        \"train_args\": config,\n",
    "                    },\n",
    "                    os.path.join(temp_checkpoint_dir, \"checkpoint.pt\"),\n",
    "                )\n",
    "                checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "                train.report(metrics=results_dict, checkpoint=checkpoint)\n",
    "        else:\n",
    "            train.report(metrics=results_dict, checkpoint=None)\n",
    "\n",
    "        adjust_learning_rate(model_optim, epoch + 1, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run Training ===\n",
    "print(\"Starting Ray Train job...\")\n",
    "result = trainer.fit()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Post-Training ===\n",
    "if result.best_checkpoints:\n",
    "    best_checkpoint_path = None\n",
    "    if not config[\"train_only\"] and \"val/loss\" in result.metrics_dataframe:\n",
    "        best_checkpoint = result.get_best_checkpoint(metric=\"val/loss\", mode=\"min\")\n",
    "        if best_checkpoint:\n",
    "            best_checkpoint_path = best_checkpoint.path\n",
    "    elif \"train/loss\" in result.metrics_dataframe:  # Fallback or if train_only\n",
    "        best_checkpoint = result.get_best_checkpoint(metric=\"train/loss\", mode=\"min\")\n",
    "        if best_checkpoint:\n",
    "            best_checkpoint_path = best_checkpoint.path\n",
    "\n",
    "    if best_checkpoint_path:\n",
    "        print(\"Best checkpoint found:\")\n",
    "        print(f\"  Directory: {best_checkpoint_path}\")\n",
    "    else:\n",
    "        print(\"Could not retrieve the best checkpoint based on available metrics.\")\n",
    "else:\n",
    "    print(\"No checkpoints were saved during training.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
