{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLinear model validation using offline batch inference\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/üöÄ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-timeseries\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "This tutorial demonstrates how to perform batch inference using the DLinear model and Ray Data.\n",
    "We load the model checkpoint, prepare the test data, run inference in batches, and evaluate the performance.\n",
    "\n",
    "Note that this notebook requires `notebook/01-Distributed_Training.ipynb` to run in order to fetch pre-trained model artifacts.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-timeseries/refs/heads/main/images/batch_inference.png\" width=800>\n",
    "\n",
    "\n",
    "The above figure illustrates how different blocks of data can be processed concurrently at various stages of the pipeline. This parallel execution maximizes resource utilization and throughput.\n",
    "\n",
    "Note that this diagram is a simplification for various reasons:\n",
    "\n",
    "* Only one worker is processing each data pipeline stage\n",
    "* Backpressure mechanisms may throttle upstream operators to prevent overwhelming downstream stages\n",
    "* Dynamic repartitioning often occurs as data moves through the pipeline, changing block counts and sizes\n",
    "* Available resources change as the cluster autoscales\n",
    "* System failures may disrupt the clean sequential flow shown in the diagram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Ray Data Streaming Execution</b> \n",
    "\n",
    "‚ùå **Traditional batch execution** (non-streaming like Spark without pipelining, Sagemaker Batch):\n",
    "- Reads the entire dataset into memory or a persistent intermediate format\n",
    "- Only then starts applying transformations, such as `.map`, `.filter`, etc.\n",
    "- Higher memory pressure and startup latency\n",
    "\n",
    "‚úÖ **Streaming execution** (Ray Data):\n",
    "- Starts processing blocks as they load, without waiting for the entire dataset\n",
    "- Reduces memory footprint (preventing OOMs) and speeds up time to first output\n",
    "- Increases resource utilization by reducing idle time\n",
    "- Enables online-style inference pipelines with minimal latency\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-timeseries/refs/heads/main/images/streaming.gif\" width=700>\n",
    "\n",
    "**Note**: Ray Data isn't a real-time stream processing engine like Flink or Kafka Streams. Instead, it's batch processing with streaming execution, which is especially useful for iterative ML workloads, ETL pipelines, and preprocessing before training or inference. Ray typically has a [**2-17x throughput improvement**](https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker#-results-of-throughput-from-experiments) over solutions like Spark and Sagemaker Batch Transform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's start by setting up the environment and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "from data_factory import data_provider\n",
    "from metrics import metric\n",
    "from model import DLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up the DLinear model configiration as well as job configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"checkpoint_path\": None,  # FIXME: REQUIRED: Update this path\n",
    "    \"num_data_workers\": 1,\n",
    "    \"features\": \"S\",\n",
    "    \"target\": \"OT\",\n",
    "    \"smoke_test\": False,\n",
    "    \"seq_len\": 96,\n",
    "    \"label_len\": 48,\n",
    "    \"pred_len\": 96,\n",
    "    \"individual\": False,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_predictor_replicas\": 4,\n",
    "}\n",
    "\n",
    "\n",
    "def _process_config(config: dict) -> dict:\n",
    "    \"\"\"Helper function to process and update configuration.\"\"\"\n",
    "    # Configure encoder input size based on task type.\n",
    "    if config[\"features\"] == \"M\" or config[\"features\"] == \"MS\":\n",
    "        config[\"enc_in\"] = 7  # ETTh1 has 7 features when multi-dimensional prediction is enabled\n",
    "    else:\n",
    "        config[\"enc_in\"] = 1\n",
    "\n",
    "    # Ensure paths are absolute.\n",
    "    config[\"checkpoint_path\"] = os.path.abspath(config[\"checkpoint_path\"])\n",
    "\n",
    "    config[\"num_gpus_per_worker\"] = 1.0\n",
    "\n",
    "    config[\"train_only\"] = False  # Load test subset\n",
    "    return config\n",
    "\n",
    "\n",
    "# Set derived values.\n",
    "config = _process_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingest\n",
    "\n",
    "First, let's load the test dataset as a Ray Data Dataset. We will use `.show(1)` to trigger the execution for a single row,\n",
    "since Ray Data is lazily evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "ds = data_provider(config, flag=\"test\")\n",
    "ds.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the Predictor class. It loads the trained DLinear model from a checkpoint and\n",
    "processes input batches to produce predictions. The __call__ method is used to perform inference\n",
    "on a given batch of numpy arrays.\n",
    "\n",
    "Ray Data's actor-based processing enables you to only load the model weights and transfer it to GPU only once and reuse it across batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    \"\"\"Actor class for performing inference with the DLinear model.\"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint_path: str, config: dict):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load model from checkpoint.\n",
    "        self.model = DLinear(config).float()\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict:\n",
    "        \"\"\"Process a batch of data for inference (numpy batch format).\"\"\"\n",
    "        # Convert input batch to tensor.\n",
    "        batch_x = torch.from_numpy(batch[\"x\"]).float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(batch_x)  # Shape (N, pred_len, features_out)\n",
    "\n",
    "        # Determine feature dimension based on config.\n",
    "        f_dim = -1 if self.config[\"features\"] == \"MS\" else 0\n",
    "        outputs = outputs[:, -self.config[\"pred_len\"] :, f_dim:]\n",
    "        outputs_np = outputs.cpu().numpy()\n",
    "\n",
    "        # Extract the target part from the batch.\n",
    "        batch_y = batch[\"y\"]\n",
    "        batch_y_target = batch_y[:, -self.config[\"pred_len\"] :]\n",
    "\n",
    "        return {\"predictions\": outputs_np, \"targets\": batch_y_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    Predictor,\n",
    "    fn_constructor_kwargs={\"checkpoint_path\": config[\"checkpoint_path\"], \"config\": config},\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    concurrency=config[\"num_predictor_replicas\"],\n",
    "    num_gpus=config[\"num_gpus_per_worker\"],\n",
    "    batch_format=\"numpy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we do some minor post-processing to get the results in the dimenions we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_items(item: dict) -> dict:\n",
    "    # Squeeze singleton dimensions for predictions and targets if necessary \n",
    "    if item[\"predictions\"].shape[-1] == 1:\n",
    "        item[\"predictions\"] = item[\"predictions\"].squeeze(-1)\n",
    "    if item[\"targets\"].shape[-1] == 1:\n",
    "        item[\"targets\"] = item[\"targets\"].squeeze(-1)\n",
    "    return item\n",
    "\n",
    "\n",
    "ds = ds.map(postprocess_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to execute all of these lazy steps and materialize it into memory using `take_all()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the lazy execution of the entire Ray pipeline.\n",
    "all_results = ds.take_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the results in memory, we can calculate some validation metrics for our trained DLinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate predictions and targets from all batches.\n",
    "all_predictions = np.concatenate([item[\"predictions\"] for item in all_results], axis=0)\n",
    "all_targets = np.concatenate([item[\"targets\"] for item in all_results], axis=0)\n",
    "\n",
    "# Compute evaluation metrics.\n",
    "mae, mse, rmse, mape, mspe, rse = metric(all_predictions, all_targets)\n",
    "\n",
    "print(\"\\n--- Test Results ---\")\n",
    "print(f\"MSE: {mse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAPE: {mape:.3f}\")\n",
    "print(f\"MSPE: {mspe:.3f}\")\n",
    "print(f\"RSE: {rse:.3f}\")\n",
    "\n",
    "print(\"\\nOffline inference finished!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
