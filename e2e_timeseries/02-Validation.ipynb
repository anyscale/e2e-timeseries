{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Inference with DLinear and Ray Data\n",
    "\n",
    "This tutorial demonstrates how to perform batch inference using the DLinear model and Ray Data.\n",
    "We load the model checkpoint, prepare the test data, run inference in batches, and evaluate the performance.\n",
    "\n",
    "\n",
    "Let's start by setting up the environment and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "from e2e_timeseries.data_provider.data_factory import data_provider\n",
    "from e2e_timeseries.models import DLinear\n",
    "from e2e_timeseries.utils.metrics import metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell parses command-line arguments needed for the inference,\n",
    "sets up the model configuration, and converts file paths to absolute paths.\n",
    "It also determines if the GPU should be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"checkpoint_path\": None,  # FIXME: REQUIRED: Update this path\n",
    "    \"root_path\": \"./e2e_timeseries/dataset/\",\n",
    "    \"data_path\": \"ETTh1.csv\",\n",
    "    \"num_data_workers\": 1,\n",
    "    \"features\": \"S\",\n",
    "    \"target\": \"OT\",\n",
    "    \"smoke_test\": False,\n",
    "    \"seq_len\": 96,\n",
    "    \"label_len\": 48,\n",
    "    \"pred_len\": 96,\n",
    "    \"individual\": False,\n",
    "    \"enc_in\": 1,  # Will be set by _process_config\n",
    "    \"batch_size\": 64,\n",
    "    \"num_predictor_replicas\": 1,\n",
    "    # num_gpus_per_worker will be set by _process_config\n",
    "}\n",
    "\n",
    "\n",
    "def _process_config(config: dict) -> dict:\n",
    "    \"\"\"Helper function to process and update configuration.\"\"\"\n",
    "    # Configure encoder input size based on task type\n",
    "    if config[\"features\"] == \"M\" or config[\"features\"] == \"MS\":\n",
    "        config[\"enc_in\"] = 7  # ETTh1 has 7 features when multiple features are used\n",
    "    else:\n",
    "        config[\"enc_in\"] = 1\n",
    "\n",
    "    # Ensure paths are absolute\n",
    "    config[\"root_path\"] = os.path.abspath(config[\"root_path\"])\n",
    "    config[\"data_path\"] = os.path.abspath(os.path.join(config[\"root_path\"], config[\"data_path\"]))\n",
    "    # Ensure checkpoint_path is absolute if it's provided and not None\n",
    "    if config.get(\"checkpoint_path\"):\n",
    "        config[\"checkpoint_path\"] = os.path.abspath(config[\"checkpoint_path\"])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available, using GPU and setting num_gpus_per_worker to 1.0\")\n",
    "        config[\"num_gpus_per_worker\"] = 1.0\n",
    "    else:\n",
    "        print(\"CUDA is not available, using CPU and setting num_gpus_per_worker to 0.0\")\n",
    "        config[\"num_gpus_per_worker\"] = 0.0\n",
    "\n",
    "    config[\"train_only\"] = False  # load test subset\n",
    "    return config\n",
    "\n",
    "\n",
    "# Set derived values\n",
    "config = _process_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This cell defines the main function which ties together the entire inference pipeline:\n",
    "- It parses configuration parameters\n",
    "- Initializes Ray\n",
    "- Loads test data using the data provider\n",
    "- Applies the Predictor for batch inference\n",
    "- Post-processes the results and computes evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "ds = data_provider(config, flag=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the Predictor class. It loads the trained DLinear model from a checkpoint and\n",
    "processes input batches to produce predictions. The __call__ method is used to perform inference\n",
    "on a given batch of numpy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    \"\"\"Actor class for performing inference with the DLinear model.\"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint_path: str, config: dict):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load model from checkpoint\n",
    "        self.model = DLinear.Model(config).float()\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict:\n",
    "        \"\"\"Process a batch of data for inference (numpy batch format).\"\"\"\n",
    "        # Convert input batch to tensor\n",
    "        batch_x = torch.from_numpy(batch[\"x\"]).float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(batch_x)  # Shape (N, pred_len, features_out)\n",
    "\n",
    "        # Determine feature dimension based on config\n",
    "        f_dim = -1 if self.config[\"features\"] == \"MS\" else 0\n",
    "        outputs = outputs[:, -self.config[\"pred_len\"] :, f_dim:]\n",
    "        outputs_np = outputs.cpu().numpy()\n",
    "\n",
    "        # Extract the target part from the batch\n",
    "        batch_y = batch[\"y\"]\n",
    "        batch_y_target = batch_y[:, -self.config[\"pred_len\"] :]\n",
    "\n",
    "        return {\"predictions\": outputs_np, \"targets\": batch_y_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    Predictor,\n",
    "    fn_constructor_kwargs={\"checkpoint_path\": config[\"checkpoint_path\"], \"config\": config},\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    concurrency=config[\"num_predictor_replicas\"],\n",
    "    num_gpus=config[\"num_gpus_per_worker\"],\n",
    "    batch_format=\"numpy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_items(item: dict) -> dict:\n",
    "    # Squeeze singleton dimensions for predictions and targets if necessary\n",
    "    if item[\"predictions\"].shape[-1] == 1:\n",
    "        item[\"predictions\"] = item[\"predictions\"].squeeze(-1)\n",
    "    if item[\"targets\"].shape[-1] == 1:\n",
    "        item[\"targets\"] = item[\"targets\"].squeeze(-1)\n",
    "    return item\n",
    "\n",
    "\n",
    "ds = ds.map(postprocess_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the lazy execution of the Ray pipeline\n",
    "all_results = ds.take_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate predictions and targets from all batches\n",
    "all_predictions = np.concatenate([item[\"predictions\"] for item in all_results], axis=0)\n",
    "all_targets = np.concatenate([item[\"targets\"] for item in all_results], axis=0)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "mae, mse, rmse, mape, mspe, rse = metric(all_predictions, all_targets)\n",
    "\n",
    "print(\"\\n--- Test Results ---\")\n",
    "print(f\"MSE: {mse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAPE: {mape:.3f}\")\n",
    "print(f\"MSPE: {mspe:.3f}\")\n",
    "print(f\"RSE: {rse:.3f}\")\n",
    "\n",
    "print(\"\\nOffline inference finished!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
