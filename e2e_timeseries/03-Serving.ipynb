{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online serving for DLinear model using Ray Serve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "import aiohttp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from fastapi import FastAPI\n",
    "from model import DLinear\n",
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_NAME = \"dlinear-ett-server\"\n",
    "\n",
    "# Create a FastAPI app that we can use to add endpoints to our Serve deployment.\n",
    "app = FastAPI(title=\"DLinear\", description=\"predict future oil temperatures\", version=\"0.1\")\n",
    "\n",
    "\n",
    "# FIXME: update GPU usage later\n",
    "@serve.deployment(num_replicas=1, ray_actor_options={\"num_cpus\": 1, \"num_gpus\": 0})\n",
    "@serve.ingress(app)\n",
    "class DLinearModelServe:\n",
    "    def __init__(self, model_checkpoint_path: str | None = None):\n",
    "        checkpoint = torch.load(model_checkpoint_path, map_location=torch.device(\"cpu\"))  # Load to CPU first\n",
    "        self.args = checkpoint[\"train_args\"]\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Load model from checkpoint.\n",
    "        self.model = DLinear(self.args).float()\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(f\"Model loaded successfully from {model_checkpoint_path}\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    @serve.batch(max_batch_size=32, batch_wait_timeout_s=0.1)\n",
    "    async def predict_batch(self, batch_x: list[list[float]]) -> list[list[float]]:\n",
    "        \"\"\"\n",
    "        Expects a list of series, where each series is a 1D list of floats/integers.\n",
    "        e.g., [[0.1, 0.2, ..., 0.N], [0.3, 0.4, ..., 0.M]]\n",
    "        Each series is a 1D list of floats/integers.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert list of 1D series to a 2D numpy array (batch_size, seq_len).\n",
    "        batch_x = np.array(batch_x, dtype=np.float32)\n",
    "        batch_x = torch.from_numpy(batch_x).float().to(self.device)\n",
    "\n",
    "        # Ensure batch_x is 3D: (batch_size, seq_len, num_features)\n",
    "        # For univariate 'S' models, num_features will be 1.\n",
    "        if batch_x.ndim == 2:\n",
    "            batch_x = batch_x.unsqueeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(batch_x)\n",
    "            # Output shape: (batch_size, pred_len, features_out)\n",
    "\n",
    "        # Slice to get the prediction length part of the output.\n",
    "        # The [:, :, :] part takes all output features.\n",
    "        # For 'S' (single-feature) forecasting, DLinear typically outputs 1 feature.\n",
    "        # For 'M' (multi-feature) forecasting, DLinear typically outputs multiple features.\n",
    "        outputs = outputs[:, -self.args[\"pred_len\"] :, :]\n",
    "\n",
    "        # If 'S' (single feature forecasting) and the model's output for that single\n",
    "        # feature has an explicit last dimension of 1, squeeze it.\n",
    "        # This makes the output a list of 1D series (list of lists of floats).\n",
    "        if outputs.shape[-1] == 1:\n",
    "            outputs = outputs.squeeze(-1)  # Shape: (batch_size, pred_len)\n",
    "\n",
    "        outputs_list = outputs.cpu().numpy().tolist()\n",
    "        return outputs_list\n",
    "\n",
    "    @app.post(\"/predict\")\n",
    "    async def predict_endpoint(self, request: Request):\n",
    "        \"\"\"\n",
    "        Expects a JSON body which is a list of floats/integers.\n",
    "        e.g., [0.1, 0.2, ..., 0.N]\n",
    "        where N must be equal to self.args.seq_len.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_data = await request.json()\n",
    "            if not isinstance(input_data, list):\n",
    "                return {\"error\": \"Invalid input. JSON list of numbers expected.\"}\n",
    "            if len(input_data) != self.args[\"seq_len\"]:\n",
    "                return {\"error\": f\"Invalid series length. Expected {self.args['seq_len']}, got {len(input_data)}.\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Failed to parse JSON request: {str(e)}\"}\n",
    "\n",
    "        # Pass the single list input_data, wrapped in another list, to predict_batch.\n",
    "        # Ray Serve's @serve.batch will handle collecting these into a batch for predict_batch.\n",
    "        # The await call will return the specific result for this input_data.\n",
    "        single_prediction_output = await self.predict_batch(input_data)\n",
    "\n",
    "        # single_prediction_output is expected to be a list[float] (the prediction for one series)\n",
    "        return single_prediction_output\n",
    "\n",
    "    # Expose get_seq_len as a GET endpoint.\n",
    "    @app.get(\"/seq_len\")\n",
    "    async def get_sequence_length(self):\n",
    "        return {\"seq_len\": self.args[\"seq_len\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"checkpoint_path\": None,  # FIXME: REQUIRED: Update this path\n",
    "}\n",
    "\n",
    "checkpoint_path = os.path.abspath(config[\"checkpoint_path\"])\n",
    "\n",
    "\n",
    "def serve_model(model_checkpoint_path_arg: str):\n",
    "    dlinear_app = DLinearModelServe.bind(model_checkpoint_path=model_checkpoint_path_arg)\n",
    "\n",
    "    # The route_prefix will apply to all routes within the FastAPI app.\n",
    "    serve.run(dlinear_app, name=DEPLOYMENT_NAME, route_prefix=\"/predict_dlinear\")\n",
    "    print(f\"DLinear model deployment '{DEPLOYMENT_NAME}' is running with FastAPI app.\")\n",
    "    print(\"  Prediction endpoint: http://127.0.0.1:8000/predict_dlinear/predict\")\n",
    "    print(\"  Sequence length endpoint: http://127.0.0.1:8000/predict_dlinear/seq_len\")\n",
    "\n",
    "    print(\"\\nTo stop the server, press Ctrl+C in the terminal where it's running.\")\n",
    "\n",
    "\n",
    "serve_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_serve():\n",
    "    # --- Example Client Code (can be run in a separate script or after serve starts) ---\n",
    "\n",
    "    # Base URL for the service.\n",
    "    base_url = \"http://127.0.0.1:8000/predict_dlinear\"\n",
    "    seq_len_url = f\"{base_url}/seq_len\"\n",
    "    predict_url = f\"{base_url}/predict\"\n",
    "\n",
    "    # Get the proper seq_len for the deployed model.\n",
    "    response = requests.get(seq_len_url)\n",
    "    response.raise_for_status()\n",
    "    seq_len_data = response.json()\n",
    "    seq_len = seq_len_data.get(\"seq_len\")\n",
    "\n",
    "    # Load sample data for demonstration purposes.\n",
    "    df = pd.read_csv(\"s3://air-example-data/electricity-transformer/ETTh2.csv\")\n",
    "    ot_series = df[\"OT\"].tolist()\n",
    "\n",
    "    # Create a single sample request from the loaded data.\n",
    "    sample_input_series = ot_series[:seq_len]\n",
    "    sample_request_body = sample_input_series\n",
    "\n",
    "    print(\"\\n--- Sending Single Synchronous Request to /predict endpoint ---\")\n",
    "    response = requests.post(predict_url, json=sample_request_body)\n",
    "    response.raise_for_status()\n",
    "    prediction = response.json()\n",
    "    print(f\"Prediction (first 5 values): {prediction[:5]}\")\n",
    "\n",
    "    print(\"\\n--- Sending Batch Asynchronous Requests to /predict endpoint ---\")\n",
    "    sample_input_list = [sample_input_series] * 100  # Use identical requests\n",
    "\n",
    "    async def fetch(session, url, data):\n",
    "        async with session.post(url, json=data) as response:\n",
    "            return await response.json()\n",
    "\n",
    "    async def fetch_all_concurrently(requests_to_send: list):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = [fetch(session, predict_url, input_data) for input_data in requests_to_send]\n",
    "            responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            return responses\n",
    "\n",
    "    asyncio.run(fetch_all_concurrently(sample_input_list))\n",
    "\n",
    "\n",
    "test_serve()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
